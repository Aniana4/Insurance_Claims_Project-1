---
title: "Primer Analisis Data Set"
author: "Montse Figueiro"
date: "26 de julio de 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

Nos da un resumen de como estan medidas las diferentes variables, mostrando:
* Minimo, **min()**
* Primer cuartil
* Mediana
* Tercer cuartil
* Máximo, **max()**

La diferencia entre el mínimo y el máximo es el rango. range() and diff() te permitiría analizar esa diferencia.

Los cuartiles dividen el dataset en 4 partes, cada uno con el mismo número de valores.

La diferencia entre cuartil 1 y cuartil 3 es Rango intercuartil (IQR), se puede calcular con la función **IQR()** 


**quantile()** nos devuelve los cinco números de summary.
**quantile(usedcars$price,probs=c(0.01,0.99))** podemos sacar los quantiles que queramos, aquí por ejemplo en el 1% y en el 99%.
**quantile(usedcars$price,seq(from=0,to=1,by=0.20))** aquí saco el 0%,20%.....

###Interpretación

Esto nos ayuda a ver la dispersión de los datos. El dataset nos dice que el mínimo es 3800 y el máximo 21992, la diferencia entre el mínimo y el Q1 es 7000, la diferencia entre el máximo y el Q3 son 7000 tambien. En cambio la diferencia entre Q1 y Q3 que es el 50% del medio son 2000. Los datos están más estrechamente agrupados en el centro, esto es típico en una distribución normal.

Esto explica porque la media es mucho más grande que la mediana, porque la media es sensible a los valores extremos.

##BOXPLOTS - visualizando los datos

El boxplot muestra los cinco números de summary usando lineas horizontales. La caja muestra Q1, mediana y Q3, leyendo desde abajo hacia arriba. La mediana es la linea en negrita. El mínimo y el máximo está representado por rayas discontinuas. Solo permite extender las rayas discontinuas hasta 1.5 veces el IQR, debajo de Q1 o encima de Q3.

Por ejemplo:
Q1 es 26
Q3 es 56

IQR es 30 (diferencia entre Q3 y Q1)

Q3 + 1.5 * 30 = 101 (no puede llegar hasta 120 que es el máximo)
Q1 - 1.5 * 30 = -9 (llegará hasta el mínimo que es 2)


**boxplot(cars$dist="Dist Cars",ylab="Km")**

```{}
summary(cars$dist)
boxplot(cars$dist,main="Dist Cars",ylab="Km")
```

##HISTOGRAMAS

Divide los valores de las variables en **bins** columnas, un boxplot requiere cada una de las 4 porciones contenga el mismo número de valores. En contraste el histograma usa cualquier número de barras de identico ancho y permite contener dentro diferente número de valores.

```{r}
hist(cars$dist,main="Distance Cars",xlab="Km")

```
Las barras indican la frecuencia de los valores, es este ejemplo vemos que lo más frecuente es que el coche para frenar recorra una distancia de entre 20 y 40 metros.

El histograma tiene una oblicuidad hacia la derecha.

##Distribución NORMAL

###Varianza y Desviación Standar

varianza se define como la media de la suma de las diferencias entre cada valor y la media al cuadrado.

Var= (sum((x-Media)^2))/n
Std= sqrt(Var)
R utiliza n-1 (sample variance), excepto en dataset muy pequeños la diferencia es mínima.

```{r}
var(cars$dist)
sd(cars$dist)
```

Cuando la varianza es muy grande esto indica que los datos se difunden muy ampliamente alrededor de la media. La desviación standar te indica para cada valor cuanto difiere de la media.

###Variables Categóricas

Cuando importamos los datos como astringsAsFactors = FALSE, R deja las variables como character en lugar de convertirlas en factores.
Podemos considerar poner los años como categórica, aunque está como entero cada año es una categoría.

Este tipo de datos se examina usando tablas en lugar de estadísticas.

One-way table : presenta una sola variable categórica.
```{r}
head(iris)
table(iris$Species)
```
Nos hace una lista de las diferentes categorías y cuantos valores hay dentro de cada una.

R puede calcular la tabla de proporciones:
```{r}
species_table <- table(iris$Species)
prop.table(species_table)
species_pct <- prop.table(species_table) *100
round(species_pct,digits=1)
```
Así podemos ver para las diferentes categorías cual es la más frecuente.

###Analizando la Moda
En estadística la moda es el valor que ocurre con más frecuencia. Esta se utiliza frecuentemente en datos categóricos, ya que no se puede usar la media o la mediana.

Una variable puede tener una moda o más
* unimodal
* bimodal
* multimodal

Es mejor relacionar la moda con otras categorias. Hay alguna categoría que domina a otra? Si los colores de coche son plata y negro, considero que estamos hablando de coches de lujo? o coches económicos, cuales se venden con menos opciones de color?
En un histograma la moda sería la barra más alta.

##RELACIÓN ENTRE VARIABLES

###SCATTERPLOT

Es un diagrama que representa la relación entre dos variables mediante circulos,  una variable se representa en el eje "x" y otra variables en el eje "y". 

```{}
plot(x=cars$dist,y=cars$speed,main="Scatterplot of Speed and Distance",ylab="Distance",xlab="Speed")
```
Vemos una relación entre la velocidad y la Distancia recorrida. Tienen una relación positiva, el patrón de circulos es ascendente.
Podría ser una relación negativa si la linea fuera descendiente. Y no tendrían nada que ver si la linea fuera plana.

La **Correlación** mide la relación entre dos variables.

###Tabulación Cruzada

Mide la relación entre dos variables. Como los valores de una variable varian en función de otra variable. El formato es una tabla donde las filas son los niveles de una variable mientras las columna son los niveles de la otra

**CrossTable()**

library(gmodels)
CrossTable(x=train_sample$Cat1,y=train_sample$Cat2)


##Clasificación usando Nearest Neighbors

Mide la similitud de dos muestras midiendo distancias. Se ha utilizado por ejemplo:

* Reconocimiento facial en imagenes y videos
* Predecir cuando una persona va a disfrutar una película que le ha sido recomendada (Netflix)
* Identificar patrones en datos genéticos al detectar una específica proteína.

Por lo general este método de clasificación es bueno cuando la relación entre las variables y las clases objetico es alto, complicado y difícil de entender. Si no hay una clara distinción entre grupos el algorítmo es largo y no es bueno identificando los límites.

###The KNN algorithm

|Fortalezas                  |                               Debilidades  |
|----------------------------|--------------------------------------------|
|Simple y efectivo | No produce un modelo, lo que lo limita para encontrar nuevos conocimientos en las relaciones entre las variables|
|No hace suposiciones sobre la distribucion de los datos subyacente | Fase de clasificación lenta|
| Fase de entrenamiento rápida | Requiere mucha memoria |
| | Variables nominales y missing data requieren procesamiento adicional |

kNN empieza con un training dataset con observaciones classificado en diferentes categorias, y etiquetadas por una variable nominal. Tenemos también un test dataset sin etiquetar que tiene el mismo número de variables que el training data. Para cada observación en el test dataset kNN identifica **k** observaciones en training dataset que son "vecinos", **k** es un entero especificado con anterioridad. A la observación del test datset se le asigna la clase de la mayoria de los k vecinos.

```{r}
ingredient <- c("apple","bacon","banana","carrot","celery","cheese")
sweetness <- c(10,1,10,7,3,1)
crunchiness <- c(9,4,1,10,10,1)
foodtype <- c("fruit","protein","fruit","vegetable","vegetable","protein") 
df <- data.frame(ingredient,sweetness,crunchiness,foodtype)
df
str(df)
```
En este ejemplo se han clasificado las comidas según unas variables, en 3 clases, fruta, carne o verdura.

En este ejemplo solo tenemos 2 variables con lo que podemos representar los datos con un scatterplot:
```{r}
plot(x=df$sweetness,y=df$crunchiness)
```
En este ejemplo hay muy pocas observaciones y no se pueden ver agrupaciones.

###Calculo Distancia

El algoritmo kNN usa la distancia Euclidean. La distancia entre dos observaciones **p** y **q** es la raíz cuadrada de la suma de las distancias al cuadrado.
p1 es el valor de la variable 1 para p
q1 es el valor de la variable 1 para q

dist(p,q)=sqrt(sum((p1-q1)^2+(p2-q2)^2.....))

La formula compara los valores para cada variable. Por ejemplo entre el tomate y la banana o entre el tomate y el queso...

Cuando calculemos todas las distancias del tomate al resto de ingredientes, veremos con que ingrediente tiene menos distancia, se llama clasificación 1NN porque k=1, por ejemplo, si el más cercano es la naranja, clasificará al tomate como "fruit". 
Si usamos kNN con k=3, lleva a cabo una votación entre los 3 vecinos más cercanos, los tres más cercanos son naranja, uva y cacahuete, 2 de 3 son frutas, el tomate lo clasificará como "fruit".

###Elección de **k**

Determinar un número correcto de vecinos determinará como de bueno será el modelo para generalizar a datos futuros. 
Escoger una k muy grande reduce el impacto de la varianza causada por el ruido, pero corre el riesgo de ignorar pequeños patrones.

Ejemplo: Tomar una k igual al nº de observaciones, cada observación estaría representada en el voto final. 

Tomar una k=1 implica que permita ruido y outliers, que influencian la clasificación de los ejemplos. Algunas de las observaciones pueden estar mal clasificadas.

El mejor valor para **k** es alguno entre estos dos.

Valores pequeños permiten más complejas decisiones, que mas cuidadosamente encaja en el training data.

Normalmente k está entre 3 y 10. Una práctica común es la raíz cuadrada del número de ejemplos del training dataset. En el ejemplo anterior teníamos 15 ingredientes, 3.87.

Una alternativa es usar diferentes **k** en diferentes test datasets y elegir uno que nos dé la mejor clasificación. si los datos tienen mucho ruido y son muy grande el número de observaciones, la elección de la **k** puede ser menos importante.

###Preparando datos para usar kNN

