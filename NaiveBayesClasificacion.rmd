---
title: "Clasificación con Naive Bayes"
author: "Montse Figueiro & Aniana González"
date: "20 de octubre de 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## CLASIFICACIÓN CON NAIVE BAYES

El método de clasificación Naive Bayes asume que todas las variables son independientes. El algoritmo de Bayes trabaja de la siguiente forma:

- Convierte el dataset en una tabla de frecuencias, para cada tipo de vehículo el número de veces que aparece con clasificación 1 o 0.
- Crea una tabla con las probabilidades a partir de las frecuencias.
- Calcula la probabilidad para cada clase, tendremos dos columnas 0 y 1 y los valores serán las probabilidades para cada observación, el valor más grande es el outcome de la predicción.

###carga de Ficheros

###Carga Fichero

```{r}
library(data.table)
library(e1071)
train_completo <- fread("train_set.csv")
datadf <- read.csv("datos_Mice.csv")
```

###Cambiamos variables del fichero train
```{r}
library(plyr)
memory.limit(60000)
train_completo <- as.data.frame(train_completo)
train_completo[6:20] <- lapply(train_completo[6:20], as.factor) 
train_completo$NVCat <- as.factor(train_completo$NVCat)
train_completo$OrdCat <- as.factor(train_completo$OrdCat)
```

```{r}
cols <- c("Row_ID", "Household_ID", "Vehicle", "Calendar_Year", "Model_Year", "Blind_Make", "Blind_Model","Blind_Submodel", "Cat1", "Cat2", "Cat3", "Cat4", "Cat5", "Cat6", "Cat7", "Cat8", "Cat9", "OrdCat", "Var1", "Var2", "Var3", "Var4", "Var5", "Var6", "Var7", "Var8", "NVCat", "NVVar1", "NVVar2", "NVVar3","NVVar4", "Claim_Amount")
train_completo <- train_completo[,cols]
```
```{r}
train_completo$clasification <- as.factor(ifelse(train_completo$Claim_Amount==0,"0","1"))
train_completo <- aggregate(Claim_Amount~Household_ID+Blind_Submodel+Model_Year+Cat1+Cat2+Cat3+Cat4+Cat5+Cat6+Cat7+Cat8+Cat9+OrdCat+Var1+Var2+Var3+Var4+Var5+Var6+Var7+Var8+NVCat+NVVar1+NVVar2+NVVar3+NVVar4+clasification,train_completo,sum)
```
```{r}
train_completo[train_completo=="?"] <- NA
train_completo$ID <- paste(train_completo$Blind_Submodel,train_completo$Model_Year,train_completo$Cat1,train_completo$Cat2,train_completo$Cat3,train_completo$Cat4,train_completo$Cat5,train_completo$Cat6,train_completo$Cat7,train_completo$Cat8,train_completo$Cat9,train_completo$OrdCat,sep="")
datadf$ID <- paste(datadf$Blind_Submodel,datadf$Model_Year,datadf$Cat1,datadf$Cat2,datadf$Cat3,datadf$Cat4,datadf$Cat5,datadf$Cat6,datadf$Cat7,datadf$Cat8,datadf$Cat9,datadf$OrdCat, sep="")
train_completo$ID <- as.factor(train_completo$ID)
datadf$ID <- as.factor(datadf$ID)
datadf <- datadf[,16:26]
head(datadf)
traindf<- merge(train_completo,datadf,by="ID",all.x = TRUE)

#Reemplazamos los NA con los valores que nos ha imputado MICE
traindf$Cat1[is.na(traindf$Cat1)] <- traindf$Cat1m[is.na(traindf$Cat1)]
traindf$Cat2[is.na(traindf$Cat2)] <- traindf$Cat2m[is.na(traindf$Cat2)]
traindf$Cat3[is.na(traindf$Cat3)] <- traindf$Cat3m[is.na(traindf$Cat3)]
traindf$Cat4[is.na(traindf$Cat4)] <- traindf$Cat4m[is.na(traindf$Cat4)]
traindf$Cat5[is.na(traindf$Cat5)] <- traindf$Cat5m[is.na(traindf$Cat5)]
traindf$Cat6[is.na(traindf$Cat6)] <- traindf$Cat6m[is.na(traindf$Cat6)]
traindf$Cat7[is.na(traindf$Cat7)] <- traindf$Cat7m[is.na(traindf$Cat7)]
traindf$Cat8[is.na(traindf$Cat8)] <- traindf$Cat8m[is.na(traindf$Cat8)]
traindf$OrdCat[is.na(traindf$OrdCat)] <- traindf$OrdCatm[is.na(traindf$OrdCat)]

#Seleccionamos las columnas que necesitamos, dejando las columnas que hemos utilizado para la limpieza del archivo.
traindf <- traindf[,2:29]

#Sustitución de los Modelos con NA por "Desconocido", recordamos que al abrir el fichero con fread nos elimina el tipo de variable y tenemos que volver a pasar a factor:
traindf$Blind_Submodel<- as.character(traindf$Blind_Submodel)
traindf$Blind_Submodel[is.na(traindf$Blind_Submodel)] <- "Desconocido"
traindf$Blind_Submodel<- factor(traindf$Blind_Submodel)
prop.table(table(traindf$clasification))
```

Guardamos una copia del fichero:
```{r}
write.csv(traindf,"train_obs_unicas_completo.csv",row.names=FALSE)
```


###DIVIDIMOS EL DATASET EN TRAIN Y TEST


```{r}
idx <- sample(seq(1, 2), size = nrow(traindf), replace = TRUE, prob = c(.75, .25))
train <- traindf[idx == 1,]
test <- traindf[idx == 2,]
```

##CLASIFICACIÓN CON NAIVE BAYES

##CARACTERISTICAS DEL VEHÍCULO Y DE LA POLIZA

###Bayes con Unbalanced Data

Nuestro DataSet no está equilibrado más del 95% de las observaciones son clasification=0, esto genera problemas a la hora de calcular los modelos y clasificar puesto que cualquier modelo tenderá a poner el valor de la clase mayoritaria.

```{r}
cols <-c("Blind_Submodel","Model_Year", "Cat1", "Cat2", "Cat3", "Cat4", "Cat5", "Cat6", "Cat7", "Cat8", "Cat9", "OrdCat", "Var1", "Var2", "Var3", "Var4", "Var5", "Var6", "Var7", "Var8", "NVCat", "NVVar1", "NVVar2", "NVVar3", "NVVar4", "clasification")
bayes1 <- train[,cols]
```

Pasamos las numéricas a categóricas:
```{r}
quantiles <- as.vector(quantile(bayes1$Var1, prob = seq(0, 1, length = 11), type = 5))
quantiles2 <- as.vector(quantile(bayes1$Var2, prob = seq(0, 1, length = 11), type = 5))
quantiles3 <- as.vector(quantile(bayes1$Var3, prob = seq(0, 1, length = 11), type = 5))
quantiles4 <- as.vector(quantile(bayes1$Var4, prob = seq(0, 1, length = 11), type = 5))
quantiles5 <- as.vector(quantile(bayes1$Var5, prob = seq(0, 1, length = 11), type = 5))
quantiles6 <- as.vector(quantile(bayes1$Var6, prob = seq(0, 1, length = 11), type = 5))
quantiles7 <- as.vector(quantile(bayes1$Var7, prob = seq(0, 1, length = 11), type = 5))
quantiles8 <- as.vector(quantile(bayes1$Var8, prob = seq(0, 1, length = 11), type = 5))

bayes1$Var1cat <- cut(bayes1$Var1,breaks=c(-Inf,-2.578222, -0.9230164, -0.7141069, -0.6176872, -0.4569876, -0.3605679, -0.1195186, 0.1777756, 0.5875594, 1.447302, 5.143392,Inf),quantiles=FALSE,labels = c("A","B","C","D","E","F","G","H","I","J","K","L"))
bayes1$Var2cat <- cut(bayes1$Var2,breaks=c(-Inf,-2.493393, -1.231139, -0.9890633, -0.7296962, -0.5222025, -0.2109619,0.0484052, 0.2213167, 0.740051, 1.17233, 7.82942,Inf),labels = c("A","B","C","D","E","F","G","H","I","J","K","L"))
bayes1$Var3cat <- cut(bayes1$Var3,breaks=c(-Inf,-2.790335, -1.239933, -1.05481, -0.7771261, -0.4531614, -0.3143194, 
0.0096453, 0.3336099, 1.097241, 1.583188, 5.5633254,Inf),labels = c("A","B","C","D","E","F","G","H","I","J","K","L"))
bayes1$Var4cat <- cut(bayes1$Var4,breaks=c(-Inf,-2.508216, -1.205861, -0.9521559, -0.6984504, -0.5293134, -0.1910394, 
0.1472346, 0.3163716, 0.6546456, 1.246625, 7.589263,Inf),labels = c("A","B","C","D","E","F","G","H","I","J","K","L"))
bayes1$Var5cat <- cut(bayes1$Var5,breaks=c(-Inf,-3.350344, -1.078535, -0.8347024, -0.5968167, -0.2637767, -0.1150981, 
0.0692634, 0.384462, 0.6461363, 1.597679, 4.018167,Inf),labels = c("A","B","C","D","E","F","G","H","I","J","K","L"))
bayes1$Var6cat <- cut(bayes1$Var6,breaks=c(-Inf,-2.376657, -1.196419, -0.8934643, -0.6501645, -0.4700292, -0.2723481, 
-0.0033148, 0.3089978, 0.6411955, 1.399168, 4.584289,Inf),labels = c("A","B","C","D","E","F","G","H","I","J","K","L"))
bayes1$Var7cat <- cut(bayes1$Var7,breaks=c(-Inf,-2.778491, -1.13195, -0.9599238, -0.8616229, -0.763322, -0.5544326, 
0.2688375, 0.6128907, 0.9938067, 1.374723, 4.127148,Inf),labels = c("A","B","C","D","E","F","G","H","I","J","K","L"))
bayes1$Var8cat <- cut(bayes1$Var8,breaks=c(-Inf,-2.163042, -1.006532, -0.7759298, -0.563355919999999, -0.4176654, 
-0.2781884, -0.109470489999998, 0.1194577, 0.5016793, 1.01036, 47.35074,Inf) ,labels=c("A","B","C","D","E","F","G","H","I","J","K","L"))
bayes1$Model_Year <- as.factor(bayes1$Model_Year)
bayes1$OrdCat <- as.factor(bayes1$OrdCat)
bayes1$NVCat <- as.factor(bayes1$NVCat)
bayes1$NVVar1 <- as.factor(bayes1$NVVar1)
bayes1$NVVar2 <- as.factor(bayes1$NVVar2)
bayes1$NVVar3 <- as.factor(bayes1$NVVar3)
bayes1$NVVar4 <- as.factor(bayes1$NVVar4)
```

Creamos el Modelo
```{r}
colsbayes1 <- c("Model_Year", "Cat1", "Cat2", "Cat3", "Cat4", "Cat5", "Cat6", "Cat7", "Cat8", "Cat9", "OrdCat", "Var1cat", "Var2cat", "Var3cat", "Var4cat", "Var5cat", "Var6cat", "Var7cat", "Var8cat","NVCat","NVVar1","NVVar2","NVVar3","NVVar4")
trainbayes1 <- bayes1[,colsbayes1]
bayes1_class <- as.factor(bayes1[,"clasification"])
modelo_bayes1 <- naiveBayes(trainbayes1,bayes1_class)
```
Aplicamos el Modelo al fichero train y validamos:
```{r}
pred_bayes1 <- predict(modelo_bayes1,trainbayes1)
table(pred_bayes1,bayes1_class)
```

Tabla de Clasificación:


|    |no | si|
|----|---|----|
|no|6242858|71218|
|si|621|2|

*Nos ha clasificado 2 de clase=1 correctamente y 621 incorrecamente*
*Obtenemos un Accuracy elevado porque se trata de un dataset con una clase mayoritaria = 0 de más del 95% de los datos*

###MODELO APLICADO A TEST

Cambiamos Tipo de Variables Continuas a categóricas en el fichero Test:
```{r}
test$clasification <- ifelse(test$Claim_Amount==0,"0","1")
test_bayes <- test
test_bayes$Var1cat <- cut(test_bayes$Var1,breaks=c(-Inf,-2.578222, -0.9230164, -0.7141069, -0.6176872, -0.4569876, -0.3605679, -0.1195186, 0.1777756, 0.5875594, 1.447302, 5.143392,Inf),quantiles=FALSE,labels = c("A","B","C","D","E","F","G","H","I","J","K","L"))
test_bayes$Var2cat <- cut(test_bayes$Var2,breaks=c(-Inf,-2.493393, -1.231139, -0.9890633, -0.7296962, -0.5222025, -0.2109619,0.0484052, 0.2213167, 0.740051, 1.17233, 7.82942,Inf),labels = c("A","B","C","D","E","F","G","H","I","J","K","L"))
test_bayes$Var3cat <- cut(test_bayes$Var3,breaks=c(-Inf,-2.790335, -1.239933, -1.05481, -0.7771261, -0.4531614, -0.3143194, 0.0096453, 0.3336099, 1.097241, 1.583188, 5.5633254,Inf),labels = c("A","B","C","D","E","F","G","H","I","J","K","L"))
test_bayes$Var4cat <- cut(test_bayes$Var4,breaks=c(-Inf,-2.508216, -1.205861, -0.9521559, -0.6984504, -0.5293134, -0.1910394, 0.1472346, 0.3163716, 0.6546456, 1.246625, 7.589263,Inf),labels = c("A","B","C","D","E","F","G","H","I","J","K","L"))
test_bayes$Var5cat <- cut(test_bayes$Var5,breaks=c(-Inf,-3.350344, -1.078535, -0.8347024, -0.5968167, -0.2637767, -0.1150981, 0.0692634, 0.384462, 0.6461363, 1.597679, 4.018167,Inf),labels = c("A","B","C","D","E","F","G","H","I","J","K","L"))
test_bayes$Var6cat <- cut(test_bayes$Var6,breaks=c(-Inf,-2.376657, -1.196419, -0.8934643, -0.6501645, -0.4700292, -0.2723481, -0.0033148, 0.3089978, 0.6411955, 1.399168, 4.584289,Inf),labels = c("A","B","C","D","E","F","G","H","I","J","K","L"))
test_bayes$Var7cat <- cut(test_bayes$Var7,breaks=c(-Inf,-2.778491, -1.13195, -0.9599238, -0.8616229, -0.763322, -0.5544326, 0.2688375, 0.6128907, 0.9938067, 1.374723, 4.127148,Inf),labels = c("A","B","C","D","E","F","G","H","I","J","K","L"))
test_bayes$Var8cat <- cut(test_bayes$Var8,breaks=c(-Inf,-2.163042, -1.006532, -0.7759298, -0.563355919999999, -0.4176654, 
-0.2781884, -0.109470489999998, 0.1194577, 0.5016793, 1.01036, 47.35074,Inf) ,labels=c("A","B","C","D","E","F","G","H","I","J","K","L"))
test_bayes$NVVar1 <- as.factor(test_bayes$NVVar1)
test_bayes$NVVar2 <- as.factor(test_bayes$NVVar2)
test_bayes$NVVar3 <- as.factor(test_bayes$NVVar3)
test_bayes$NVVar4 <- as.factor(test_bayes$NVVar4)
test_bayes$Model_Year <- as.factor(test_bayes$Model_Year)
test_bayes$OrdCat <- as.factor(test_bayes$OrdCat)
```

Fichero Test separamos la variable "Clasification" en un vector aparte para poderlo cruzar con los datos que nos prediga el modelo:
```{r}
cols <- c("Model_Year", "Cat1", "Cat2", "Cat3", "Cat4", "Cat5", "Cat6", "Cat7", "Cat8", "Cat9", "OrdCat", "Var1cat", "Var2cat", "Var3cat", "Var4cat", "Var5cat", "Var6cat", "Var7cat", "Var8cat", "NVCat", "NVVar1", "NVVar2", "NVVar3", "NVVar4")
test_NB <- test_bayes[,cols]
testresult <- as.factor(test_bayes[,"clasification"])
```

Aplicamos el Modelo de Naive Bayes al fichero Test:
```{r}
pred_bayestest <- predict(modelo_bayes1,test_NB)
table(pred_bayestest,testresult)
```

Tabla de Clasificación:

|    |no | si|
|----|---|----|
|no|1699374|18294|
|si|380007|5496|


Accuracy del 81%
```{r}
((table(pred_bayestest,testresult)[1])+(table(pred_bayestest,testresult)[4]))/nrow(test)
```
*clasifica la mayor parte de las observaciones como 0*

Negative Predictive Value
```{r}
(table(pred_bayestest,testresult)[4])/((table(pred_bayestest,testresult)[4])+(table(pred_bayestest,testresult)[3]))
```

##SOLO CON LAS CARACTERÍSTICAS DEL VEHÍCULO

En éste caso no tenemos en cuenta las características de la póliza para hacer la predicción, incluimos la proporción de vehículos con y sin daños corporal para cada observación diferente:

```{r}
cols <- c( "Model_Year", "Cat1", "Cat2", "Cat3", "Cat4", "Cat5", "Cat6", "Cat7", "Cat8", "Cat9", "OrdCat", "Var1cat", "Var2cat", "Var3cat", "Var4cat", "Var5cat", "Var6cat", "Var7cat", "Var8cat","clasification")
bayes2 <- bayes1[,cols]
```

```{r}
colsbayes <- c("Model_Year", "Cat1", "Cat2", "Cat3", "Cat4", "Cat5", "Cat6", "Cat7", "Cat8", "Cat9", "OrdCat", "Var1cat", "Var2cat", "Var3cat", "Var4cat", "Var5cat", "Var6cat", "Var7cat", "Var8cat")
train_aggr <- bayes2[,colsbayes]
class_aggr <- as.factor(bayes2[,"clasification"])
modelo_bayes_aggr <- naiveBayes(train_aggr,class_aggr)
```
Aplicamos el Modelo al fichero train y validamos:
```{r}
pred_bayes_aggr <- predict(modelo_bayes_aggr,train_aggr)
table(pred_bayes_aggr,class_aggr)
```
Tabla de Clasificación:

|    |no | si  |
|----|---|-----|
|no|6243472|71220|
|si|7|0|


Aplicamos el Modelo de Naive Bayes al fichero Test:

```{r}
cols <- c("Model_Year", "Cat1", "Cat2", "Cat3", "Cat4", "Cat5", "Cat6", "Cat7", "Cat8", "Cat9", "OrdCat", "Var1cat", "Var2cat", "Var3cat", "Var4cat", "Var5cat", "Var6cat", "Var7cat", "Var8cat")
test_NB2 <- test_bayes[,cols]
pred_bayestest2 <- predict(modelo_bayes_aggr,test_NB2)
table(pred_bayestest2,testresult)
```
Tabla de Clasificación:

|    |no | si |
|----|---|----|
|no|2079379|23790|
|si|2|0|


##UNDER-SAMPLING FOR UNBALANCED DATA

Al tener suficiente número de observaciones lo más eficiente para después poder aplicar los diferentes métodos de Machine Learning es reducir las observaciones de la clase mayoritaria, en éste caso, los que no han tenido siniestro con daño corporal. 


Del fichero train seleccionamos todas las observaciones con clasificación = 1 y una muestra aleatoria de la clase mayoritaria para equilibrar el fichero:
```{r}
#El fichero lo voy a dividir en dos grupos la clase minoritaria y la mayoritaria:
train_major_class <- bayes1[bayes1$clasification=="0",]
train_minor_class <- bayes1[bayes1$clasification!="0",]
sample_major_class <- train_major_class[sample(nrow(train_major_class),nrow(train_minor_class),replace=FALSE),]
train_balanced <- rbind(train_minor_class,sample_major_class)
prop.table(table(train_balanced$clasification))
```

```{r}
cols <- c("Model_Year", "Cat1", "Cat2", "Cat3", "Cat4", "Cat5", "Cat6", "Cat7", "Cat8", "Cat9", "OrdCat", "Var1cat", "Var2cat", "Var3cat", "Var4cat", "Var5cat", "Var6cat", "Var7cat", "Var8cat","NVCat","NVVar1","NVVar2","NVVar3","NVVar4")
train_bayes_balanced <- train_balanced[,cols]
class_balanced <- as.factor(train_balanced[,"clasification"])
modelo_bayes_balanced <- naiveBayes(train_bayes_balanced,class_balanced)
```

Aplicamos el Modelo al fichero train y validamos:
```{r}
pred_bayes_balanced <- predict(modelo_bayes_balanced,train_bayes_balanced)
table(pred_bayes_balanced,class_balanced)
```
Tabla de Clasificación:

|    |no | si  |
|----|---|-----|
|no|39215|32298|
|si|32005|38922|


Aplicamos el Modelo de Naive Bayes al fichero Test:

```{r}
cols <- c("Model_Year", "Cat1", "Cat2", "Cat3", "Cat4", "Cat5", "Cat6", "Cat7", "Cat8", "Cat9", "OrdCat", "Var1cat", "Var2cat", "Var3cat", "Var4cat", "Var5cat", "Var6cat", "Var7cat", "Var8cat","NVCat","NVVar1","NVVar2","NVVar3","NVVar4")
test_NB_balanced <- test_bayes[,cols]
pred_bayestest_balanced <- predict(modelo_bayes_balanced,test_NB_balanced)
table(pred_bayestest_balanced,testresult)
```

Tabla de Clasificación:


|  |no     | si    |
|--|-------|-------|
|no|1200050|11562  |
|si|879331 |12228  |
  

###Calibración de probabilidades

```{r}
pred_bayestest_balanced <- predict(modelo_bayes_balanced,test_NB_balanced,type = "raw")
pred <- as.data.frame(pred_bayestest_balanced)
pred_50 <- as.factor(ifelse(pred$`1`>0.5,"1","0"))
table(pred_50,testresult)
pred_60 <- as.factor(ifelse(pred$`1`>0.6,"1","0"))
table(pred_60,testresult)
pred_70 <- as.factor(ifelse(pred$`1`>0.7,"1","0"))
table(pred_70,testresult)
```
